{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f30a088-b376-4f37-ad3c-0748de6b103a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import os\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing import image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5f5b078-59b4-4891-b072-efb14c7f9711",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2145 images belonging to 2 classes.\n",
      "Found 535 images belonging to 2 classes.\n",
      "Class labels: {'hyena': 0, 'not_hyene': 1}\n"
     ]
    }
   ],
   "source": [
    "# === Dataset path ===\n",
    "train_dir = r'C:\\Hope\\AI Course Tamil\\8.Deep Learning\\dataset'\n",
    "\n",
    "# === Data Augmentation ===\n",
    "\n",
    "# ğŸ§  Overfitting Prevention:\n",
    "datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    validation_split=0.2)\n",
    "\n",
    "# ğŸ¯ Generalization Improve Aagum:\n",
    "train_gen = datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(224, 224),  # MobileNetV2 input size\n",
    "    batch_size=32,\n",
    "    class_mode='binary',\n",
    "    subset='training')\n",
    "\n",
    "# ğŸ“ˆ Virtual Dataset Expansion:\n",
    "val_gen = datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='binary',\n",
    "    subset='validation')\n",
    "\n",
    "# === Print class labels ===\n",
    "print(\"Class labels:\", train_gen.class_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "267391d9-f68c-4bc3-ac4b-a8d1dfb8758f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Load Pretrained Model (MobileNetV2) ===\n",
    "base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "base_model.trainable = False  # Freeze base model layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02551863-4c63-4a38-8a70-867b5a00351b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Add custom classifier on top ===\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(64, activation='relu')(x)\n",
    "predictions = Dense(1, activation='sigmoid')(x)  # Binary classification\n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# === Compile the model ===\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3488d81-2c30-47dc-acf9-dc5e78710967",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m68/68\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 1s/step - accuracy: 0.8852 - loss: 0.2496 - val_accuracy: 0.9813 - val_loss: 0.0680\n",
      "Epoch 2/5\n",
      "\u001b[1m68/68\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 1s/step - accuracy: 0.9976 - loss: 0.0112 - val_accuracy: 0.9738 - val_loss: 0.0783\n",
      "Epoch 3/5\n",
      "\u001b[1m68/68\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m97s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 0.0076 - val_accuracy: 0.9757 - val_loss: 0.0768\n",
      "Epoch 4/5\n",
      "\u001b[1m68/68\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m95s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 0.0023 - val_accuracy: 0.9776 - val_loss: 0.0730\n",
      "Epoch 5/5\n",
      "\u001b[1m68/68\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 0.0013 - val_accuracy: 0.9794 - val_loss: 0.0712\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x2a50ecdf710>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === Train the model ===\n",
    "model.fit(train_gen, validation_data=val_gen, epochs=5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36abf0de-36c6-4316-8c01-f628b488e70c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Model saved!\n"
     ]
    }
   ],
   "source": [
    "# === Save the model ===\n",
    "model.save(r'C:\\Hope\\AI Course Tamil\\8.Deep Learning\\hyena_mobilenetv2.h5')\n",
    "print(\"âœ… Model saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "142df2ad-3faf-43f9-afa9-31fa6fe65a3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "Raw Prediction Score: 0.0000\n",
      "ğŸ¦ Prediction: Hyena Detected (100.00%)\n"
     ]
    }
   ],
   "source": [
    "# === Load and test on a single image ===\n",
    "model = tf.keras.models.load_model(r'C:\\Hope\\AI Course Tamil\\8.Deep Learning\\hyena_mobilenetv2.h5')\n",
    "\n",
    "img_path = r'C:\\Hope\\AI Course Tamil\\8.Deep Learning\\test_image.jpg'\n",
    "if not os.path.exists(img_path):\n",
    "    print(\"Image not found:\", img_path)\n",
    "else:\n",
    "    img = image.load_img(img_path, target_size=(224, 224))\n",
    "    img_array = image.img_to_array(img) / 255.0\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "\n",
    "    prediction = model.predict(img_array)\n",
    "    score = prediction[0][0]\n",
    "    percentage = score * 100\n",
    "\n",
    "    print(f\"Raw Prediction Score: {score:.4f}\")\n",
    "    if score > 0.5:\n",
    "        print(f\"ğŸ¦ Prediction: Not a Hyena ({percentage:.2f}%)\")\n",
    "    else:\n",
    "        print(f\"ğŸ¦ Prediction: Hyena Detected ({100 - percentage:.2f}%)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
